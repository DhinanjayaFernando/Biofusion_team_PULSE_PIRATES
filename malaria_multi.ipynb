{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b2aa1edb126448b3a7f0f468381b216a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8915116296a34b15a73b30e32b406b57",
              "IPY_MODEL_45240f1974b54689a511857955faac91",
              "IPY_MODEL_c2284b4156ec4f62ba4d82b8d78dceb2"
            ],
            "layout": "IPY_MODEL_9b46f1471ac14293ac216ea380dd655f"
          }
        },
        "8915116296a34b15a73b30e32b406b57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9455b4c029d436995fd6a912d681c72",
            "placeholder": "​",
            "style": "IPY_MODEL_888f305632674d7fa18ce6f635abb8dc",
            "value": "100%"
          }
        },
        "45240f1974b54689a511857955faac91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6404087b34eb437abdad86c8ae63ad6b",
            "max": 1208,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3d2d7e10305d4f3bac434a62aa1e0b3c",
            "value": 1208
          }
        },
        "c2284b4156ec4f62ba4d82b8d78dceb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a85b512e7ce54f41a66cf36d6946074a",
            "placeholder": "​",
            "style": "IPY_MODEL_7b9c6b6e68454381b407908920a1f2a5",
            "value": " 1208/1208 [01:22&lt;00:00, 15.43it/s]"
          }
        },
        "9b46f1471ac14293ac216ea380dd655f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9455b4c029d436995fd6a912d681c72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "888f305632674d7fa18ce6f635abb8dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6404087b34eb437abdad86c8ae63ad6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d2d7e10305d4f3bac434a62aa1e0b3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a85b512e7ce54f41a66cf36d6946074a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b9c6b6e68454381b407908920a1f2a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9d18101988b4cd6b43c27c1dbc84930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_438039669a74410f847623b72ef3b21e",
              "IPY_MODEL_82762ff639d742219d52e7e636360637",
              "IPY_MODEL_4eecfbe7e66a4fa09bd709ebcfe59524"
            ],
            "layout": "IPY_MODEL_2d36875e09eb426fb23e4e745286f45d"
          }
        },
        "438039669a74410f847623b72ef3b21e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5ae2820aa9342f1a884d83561e109c9",
            "placeholder": "​",
            "style": "IPY_MODEL_7daab53602914cca9652593c52e5a3ad",
            "value": "100%"
          }
        },
        "82762ff639d742219d52e7e636360637": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9439d94edc314f65853f1e9500421dde",
            "max": 120,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9aec71b43f5144a682b2c167ae8d7112",
            "value": 120
          }
        },
        "4eecfbe7e66a4fa09bd709ebcfe59524": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21ae08d33c344071a1fb76a55495f5c4",
            "placeholder": "​",
            "style": "IPY_MODEL_f16f51640d4a4f1db0735c154b5ad2f4",
            "value": " 120/120 [00:01&lt;00:00, 79.65it/s]"
          }
        },
        "2d36875e09eb426fb23e4e745286f45d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5ae2820aa9342f1a884d83561e109c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7daab53602914cca9652593c52e5a3ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9439d94edc314f65853f1e9500421dde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9aec71b43f5144a682b2c167ae8d7112": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "21ae08d33c344071a1fb76a55495f5c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f16f51640d4a4f1db0735c154b5ad2f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# CELL 1: Setup & Data Download (K Scott Mader Dataset)\n",
        "# ==========================================\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# 1. Install necessary libraries\n",
        "!pip install ultralytics -q\n",
        "!pip install pandas -q\n",
        "\n",
        "# 2. Upload kaggle.json\n",
        "print(\"Please upload your kaggle.json file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# 3. Setup Kaggle credentials\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/.kaggle.json # Fix: Ensure kaggle.json is copied as .kaggle.json\n",
        "!chmod 600 ~/.kaggle/.kaggle.json\n",
        "\n",
        "# 4. Download K Scott Mader Malaria Dataset\n",
        "print(\"Downloading K Scott Mader Malaria Dataset...\")\n",
        "!kaggle datasets download -d kmader/malaria-bounding-boxes\n",
        "!mkdir -p datasets/raw_malaria  # Create the directory before unzipping\n",
        "!unzip -q malaria-bounding-boxes.zip -d datasets/raw_malaria\n",
        "\n",
        "print(\"Download Complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "FxW-iVWMesIN",
        "outputId": "84dbec52-568b-47a8-e039-ac2a7286f8fd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your kaggle.json file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e9c0a1ab-cab4-4ab3-87b7-f8af9936a44f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e9c0a1ab-cab4-4ab3-87b7-f8af9936a44f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (2).json\n",
            "chmod: cannot access '/root/.kaggle/.kaggle.json': No such file or directory\n",
            "Downloading K Scott Mader Malaria Dataset...\n",
            "Dataset URL: https://www.kaggle.com/datasets/kmader/malaria-bounding-boxes\n",
            "License(s): Attribution-NonCommercial-ShareAlike 3.0 IGO (CC BY-NC-SA 3.0 IGO)\n",
            "malaria-bounding-boxes.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Download Complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# CELL 2: Convert JSON to YOLO Format\n",
        "# ==========================================\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "import cv2\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Define paths\n",
        "base_path = 'datasets/raw_malaria/malaria'\n",
        "images_source = os.path.join(base_path, 'images')\n",
        "json_train = os.path.join(base_path, 'training.json')\n",
        "json_test = os.path.join(base_path, 'test.json')\n",
        "\n",
        "# Output directories for YOLO\n",
        "output_path = 'datasets/malaria_yolo'\n",
        "for split in ['train', 'test']:\n",
        "    os.makedirs(f'{output_path}/images/{split}', exist_ok=True)\n",
        "    os.makedirs(f'{output_path}/labels/{split}', exist_ok=True)\n",
        "\n",
        "# Define Classes (Based on dataset analysis)\n",
        "class_map = {\n",
        "    'red blood cell': 0,\n",
        "    'trophozoite': 1,\n",
        "    'difficult': 2,\n",
        "    'ring': 3,\n",
        "    'schizont': 4,\n",
        "    'gametocyte': 5,\n",
        "    'leukocyte': 6\n",
        "}\n",
        "\n",
        "def process_json_to_yolo(json_file, split_name):\n",
        "    print(f\"Processing {split_name} data...\")\n",
        "    df = pd.read_json(json_file)\n",
        "\n",
        "    # Iterate through each image entry\n",
        "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "        # Get image info\n",
        "        img_data = row['image']\n",
        "        filename = os.path.basename(img_data['pathname'])\n",
        "\n",
        "        # Source image path (handle potential subdirectories if any, usually flat in this dataset)\n",
        "        src_img_path = os.path.join(images_source, filename)\n",
        "\n",
        "        # Check if image exists\n",
        "        if not os.path.exists(src_img_path):\n",
        "            continue\n",
        "\n",
        "        # Read image to get dimensions (needed for normalization)\n",
        "        img = cv2.imread(src_img_path)\n",
        "        if img is None: continue\n",
        "        height, width, _ = img.shape\n",
        "\n",
        "        # Copy image to YOLO folder\n",
        "        dst_img_path = f'{output_path}/images/{split_name}/{filename}'\n",
        "        shutil.copy(src_img_path, dst_img_path)\n",
        "\n",
        "        # Create Label File\n",
        "        label_filename = filename.rsplit('.', 1)[0] + '.txt'\n",
        "        label_path = f'{output_path}/labels/{split_name}/{label_filename}'\n",
        "\n",
        "        with open(label_path, 'w') as f:\n",
        "            for obj in row['objects']:\n",
        "                category = obj['category']\n",
        "                if category not in class_map: continue\n",
        "                cls_id = class_map[category]\n",
        "\n",
        "                # Extract Box (JSON format: r=row(y), c=col(x))\n",
        "                bbox = obj['bounding_box']\n",
        "                min_r, min_c = bbox['minimum']['r'], bbox['minimum']['c']\n",
        "                max_r, max_c = bbox['maximum']['r'], bbox['maximum']['c']\n",
        "\n",
        "                # Convert to YOLO (x_center, y_center, w, h) normalized\n",
        "                x_center = ((min_c + max_c) / 2) / width\n",
        "                y_center = ((min_r + max_r) / 2) / height\n",
        "                w = (max_c - min_c) / width\n",
        "                h = (max_r - min_r) / height\n",
        "\n",
        "                # Clip values to 0-1 just in case\n",
        "                x_center = max(0, min(1, x_center))\n",
        "                y_center = max(0, min(1, y_center))\n",
        "                w = max(0, min(1, w))\n",
        "                h = max(0, min(1, h))\n",
        "\n",
        "                f.write(f\"{cls_id} {x_center} {y_center} {w} {h}\\n\")\n",
        "\n",
        "# Run conversion\n",
        "process_json_to_yolo(json_train, 'train')\n",
        "process_json_to_yolo(json_test, 'test')\n",
        "\n",
        "# Create data.yaml\n",
        "data_yaml_content = f\"\"\"\n",
        "path: /content/{output_path}\n",
        "train: images/train\n",
        "val: images/test  # Using test as val for simplicity here\n",
        "test: images/test\n",
        "nc: {len(class_map)}\n",
        "names: {list(class_map.keys())}\n",
        "\"\"\"\n",
        "with open(f'{output_path}/data.yaml', 'w') as f:\n",
        "    f.write(data_yaml_content)\n",
        "\n",
        "print(\"Dataset conversion to YOLO format complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133,
          "referenced_widgets": [
            "b2aa1edb126448b3a7f0f468381b216a",
            "8915116296a34b15a73b30e32b406b57",
            "45240f1974b54689a511857955faac91",
            "c2284b4156ec4f62ba4d82b8d78dceb2",
            "9b46f1471ac14293ac216ea380dd655f",
            "b9455b4c029d436995fd6a912d681c72",
            "888f305632674d7fa18ce6f635abb8dc",
            "6404087b34eb437abdad86c8ae63ad6b",
            "3d2d7e10305d4f3bac434a62aa1e0b3c",
            "a85b512e7ce54f41a66cf36d6946074a",
            "7b9c6b6e68454381b407908920a1f2a5",
            "b9d18101988b4cd6b43c27c1dbc84930",
            "438039669a74410f847623b72ef3b21e",
            "82762ff639d742219d52e7e636360637",
            "4eecfbe7e66a4fa09bd709ebcfe59524",
            "2d36875e09eb426fb23e4e745286f45d",
            "a5ae2820aa9342f1a884d83561e109c9",
            "7daab53602914cca9652593c52e5a3ad",
            "9439d94edc314f65853f1e9500421dde",
            "9aec71b43f5144a682b2c167ae8d7112",
            "21ae08d33c344071a1fb76a55495f5c4",
            "f16f51640d4a4f1db0735c154b5ad2f4"
          ]
        },
        "id": "8q5noTqLevaP",
        "outputId": "6410eb0c-7761-4b70-a248-d74e9a3e258e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing train data...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1208 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2aa1edb126448b3a7f0f468381b216a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing test data...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/120 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b9d18101988b4cd6b43c27c1dbc84930"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset conversion to YOLO format complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# CELL 3: Define CBAM Attention (Like Platelet.ipynb)\n",
        "# ==========================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import ultralytics.nn.tasks\n",
        "\n",
        "# 1. Channel Attention Module\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_planes, ratio=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "# 2. Spatial Attention Module\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "        x = self.conv1(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# 3. CBAM Block\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, c1, kernel_size=7):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_attention = ChannelAttention(c1)\n",
        "        self.spatial_attention = SpatialAttention(kernel_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.channel_attention(x) * self.spatial_attention(x)\n",
        "\n",
        "# 4. Register CBAM\n",
        "# This allows the YAML file to find \"CBAM\"\n",
        "ultralytics.nn.tasks.CBAM = CBAM\n",
        "# Sometimes required for safe lookup\n",
        "setattr(ultralytics.nn.modules, 'CBAM', CBAM)\n",
        "\n",
        "print(\"Attention Mechanism Registered Successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXIMubDQeyGJ",
        "outputId": "1ceefb69-2c7b-489d-8253-03d5b8809f64"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Attention Mechanism Registered Successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# CELL 4: Custom Model Config with Attention\n",
        "# ==========================================\n",
        "# We insert the CBAM module into the YOLOv8n backbone\n",
        "# NOTE: The [1024] argument for CBAM matches the channels of the layer it follows\n",
        "\n",
        "yolo_cbam_yaml = \"\"\"\n",
        "# Ultralytics YOLOv8n-CBAM model\n",
        "nc: 7\n",
        "scales:\n",
        "  n: [0.33, 0.25, 1024]\n",
        "\n",
        "backbone:\n",
        "  # [from, repeats, module, args]\n",
        "  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2\n",
        "  - [-1, 1, Conv, [128, 3, 2]]  # 1-P2/4\n",
        "  - [-1, 3, C2f, [128, True]]\n",
        "  - [-1, 1, Conv, [256, 3, 2]]  # 3-P3/8\n",
        "  - [-1, 6, C2f, [256, True]]\n",
        "  - [-1, 1, Conv, [512, 3, 2]]  # 5-P4/16\n",
        "  - [-1, 6, C2f, [512, True]]\n",
        "  - [-1, 1, Conv, [1024, 3, 2]]  # 7-P5/32\n",
        "  - [-1, 3, C2f, [1024, True]]\n",
        "  - [-1, 1, SPPF, [1024, 5]]  # 9\n",
        "  - [-1, 1, CBAM, [256]]     # 10 <-- ATTENTION ADDED HERE, changed from 1024 to 256\n",
        "\n",
        "head:\n",
        "  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]\n",
        "  - [[-1, 6], 1, Concat, [1]]  # cat backbone P4\n",
        "  - [-1, 3, C2f, [512]]  # 13\n",
        "\n",
        "  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]\n",
        "  - [[-1, 4], 1, Concat, [1]]  # cat backbone P3\n",
        "  - [-1, 3, C2f, [256]]  # 16 (P3/8-small)\n",
        "\n",
        "  - [-1, 1, Conv, [256, 3, 2]]\n",
        "  - [[-1, 13], 1, Concat, [1]]  # cat head P4\n",
        "  - [-1, 3, C2f, [512]]  # 19 (P4/16-medium)\n",
        "\n",
        "  - [-1, 1, Conv, [512, 3, 2]]\n",
        "  - [[-1, 10], 1, Concat, [1]]  # cat head P5 (Connects to CBAM layer)\n",
        "  - [-1, 3, C2f, [1024]]  # 22 (P5/32-large)\n",
        "\n",
        "  - [[16, 19, 22], 1, Detect, [nc]]  # Detect(P3, P4, P5)\n",
        "\"\"\"\n",
        "\n",
        "with open(\"yolov8_cbam.yaml\", \"w\") as f:\n",
        "    f.write(yolo_cbam_yaml)\n",
        "\n",
        "print(\"yolov8_cbam.yaml created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4CnYEfFe2qt",
        "outputId": "d29cee34-8d28-48b0-e85d-c26d8bfada90"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yolov8_cbam.yaml created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# CELL 5: Train\n",
        "# ==========================================\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load the custom config\n",
        "model = YOLO('yolov8_cbam.yaml')\n",
        "\n",
        "# Train\n",
        "results = model.train(\n",
        "    data='datasets/malaria_yolo/data.yaml',\n",
        "    epochs=50,\n",
        "    imgsz=640,\n",
        "    batch=16,\n",
        "    name='malaria_attention_run',\n",
        "    patience=10\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNe9OMmLe5gf",
        "outputId": "30c4cd47-392b-45e3-c68c-29c704b03598"
      },
      "execution_count": 10,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING ⚠️ no model scale passed. Assuming scale='n'.\n",
            "Ultralytics 8.3.246 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=datasets/malaria_yolo/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8_cbam.yaml, momentum=0.937, mosaic=1.0, multi_scale=False, name=malaria_attention_run, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/malaria_attention_run, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% ━━━━━━━━━━━━ 755.1KB 27.4MB/s 0.0s\n",
            "WARNING ⚠️ no model scale passed. Assuming scale='n'.\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1      8290  CBAM                                         [256]                         \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 23        [16, 19, 22]  1    752677  ultralytics.nn.modules.head.Detect           [7, [64, 128, 256]]           \n",
            "YOLOv8_cbam summary: 137 layers, 3,020,503 parameters, 3,020,487 gradients, 8.2 GFLOPs\n",
            "\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% ━━━━━━━━━━━━ 5.4MB 104.8MB/s 0.1s\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 2058.5±1792.9 MB/s, size: 1641.8 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/malaria_yolo/labels/train... 1208 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1208/1208 258.8it/s 4.7s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/malaria_yolo/labels/train.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1792.1±1002.3 MB/s, size: 242.3 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/malaria_yolo/labels/test... 120 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 120/120 826.7it/s 0.1s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/malaria_yolo/labels/test.cache\n",
            "Plotting labels to /content/runs/detect/malaria_attention_run/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000909, momentum=0.9) with parameter groups 57 weight(decay=0.0), 67 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/detect/malaria_attention_run\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K       1/50      6.74G      5.081      3.285      3.871        668        640: 100% ━━━━━━━━━━━━ 76/76 1.1s/it 1:20\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.2s/it 4.6s\n",
            "                   all        120       5922          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/50      6.74G      3.624      1.921      3.163       1960        640: 0% ──────────── 0/76  0.5s"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K       2/50      6.76G      2.353      1.308      2.295        922        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:12\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.2it/s 1.8s\n",
            "                   all        120       5922      0.937     0.0413     0.0532     0.0264\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/50      6.76G      1.726      1.076      1.825       1585        640: 0% ──────────── 0/76  0.8s"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K       3/50      6.77G      1.561      1.006      1.573       1028        640: 100% ━━━━━━━━━━━━ 76/76 1.0it/s 1:14\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.3it/s 1.7s\n",
            "                   all        120       5922      0.951     0.0941      0.116      0.072\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/50      6.78G      1.446      1.044      1.547       1318        640: 0% ──────────── 0/76  0.5s"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K       4/50      7.65G      1.311     0.8869      1.376        748        640: 100% ━━━━━━━━━━━━ 76/76 1.0it/s 1:13\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.0it/s 2.0s\n",
            "                   all        120       5922      0.958      0.115      0.147     0.0995\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/50      3.04G      1.322     0.8289      1.222       1910        640: 0% ──────────── 0/76  0.4s"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K       5/50      5.13G      1.206     0.8083      1.278        692        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:12\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.0it/s 2.0s\n",
            "                   all        120       5922      0.958      0.122      0.145     0.0986\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/50      5.13G      1.137     0.7602      1.224       1793        640: 0% ──────────── 0/76  0.4s"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K       6/50      5.14G      1.129      0.756       1.22        485        640: 100% ━━━━━━━━━━━━ 76/76 1.0it/s 1:13\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s\n",
            "                   all        120       5922       0.82      0.147      0.166      0.115\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/50      5.15G      1.099      0.784      1.203       1617        640: 0% ──────────── 0/76  0.4s"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K       7/50      5.16G      1.051     0.7222      1.178        904        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:12\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.4it/s 2.9s\n",
            "                   all        120       5922        0.8      0.178      0.193       0.12\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/50      5.17G     0.9646     0.7392      1.161       1164        640: 0% ──────────── 0/76  0.4s"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K       8/50      5.18G      1.029        0.7      1.146        618        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:12\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.9it/s 2.1s\n",
            "                   all        120       5922      0.691      0.141      0.171       0.12\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/50      5.18G       1.02     0.6751      1.123       1867        640: 0% ──────────── 0/76  0.6s"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K       9/50      5.19G     0.9993     0.6798      1.127        659        640: 100% ━━━━━━━━━━━━ 76/76 1.0it/s 1:12\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.2it/s 1.9s\n",
            "                   all        120       5922      0.569      0.174      0.176      0.117\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/50       5.2G      1.011     0.6783      1.127       2052        640: 0% ──────────── 0/76  0.6s"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K      10/50      5.21G      0.966     0.6466      1.096        854        640: 100% ━━━━━━━━━━━━ 76/76 1.0it/s 1:13\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.0it/s 2.0s\n",
            "                   all        120       5922      0.572       0.18      0.195      0.133\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/50      5.22G     0.9632     0.6303      1.117       1609        640: 0% ──────────── 0/76  0.4s"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K      11/50      5.23G     0.9526      0.632      1.083        842        640: 100% ━━━━━━━━━━━━ 76/76 1.0it/s 1:13\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.2it/s 1.8s\n",
            "                   all        120       5922      0.868      0.138      0.177      0.121\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/50      5.24G     0.9631     0.6314      1.128       1622        640: 0% ──────────── 0/76  0.5s"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K      12/50      5.24G     0.9389     0.6217      1.067       1006        640: 100% ━━━━━━━━━━━━ 76/76 1.0it/s 1:14\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.8it/s 1.4s\n",
            "                   all        120       5922      0.374      0.219      0.231      0.163\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/50      5.25G     0.9833     0.7524      1.132       1218        640: 0% ──────────── 0/76  0.8s"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K      13/50       6.3G     0.9215     0.6073      1.053        597        640: 100% ━━━━━━━━━━━━ 76/76 1.0it/s 1:13\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.5it/s 1.6s\n",
            "                   all        120       5922       0.65      0.146      0.186      0.128\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/50      6.31G      1.018     0.6238      1.034       2410        640: 0% ──────────── 0/76  0.7s"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K      14/50      6.31G     0.9013     0.5947      1.039        963        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:12\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.4it/s 2.9s\n",
            "                   all        120       5922      0.401      0.221       0.25      0.182\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/50      6.32G     0.8744     0.5745      1.012       2017        640: 0% ──────────── 0/76  0.6s"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K      15/50      6.33G     0.9024     0.5795      1.031       1153        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:11\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.0it/s 2.0s\n",
            "                   all        120       5922      0.632      0.188       0.23       0.16\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/50      6.34G     0.8717      0.537      1.003       1729        640: 0% ──────────── 0/76  0.7s"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K      16/50      6.35G     0.8913     0.5766      1.023        854        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:12\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.2it/s 1.8s\n",
            "                   all        120       5922      0.266      0.236      0.257      0.176\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/50      6.36G     0.8435     0.5551      1.001       1613        640: 0% ──────────── 0/76  0.8s"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K      17/50      6.37G     0.8937     0.5648       1.02        674        640: 100% ━━━━━━━━━━━━ 76/76 1.0it/s 1:13\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.4it/s 1.7s\n",
            "                   all        120       5922      0.262      0.278      0.295      0.214\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/50      6.38G     0.8383     0.5283       1.01       1685        640: 0% ──────────── 0/76  0.5s"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K      18/50      6.38G     0.8895     0.5608      1.014       1093        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:12\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.0it/s 2.0s\n",
            "                   all        120       5922      0.244       0.26      0.273      0.198\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/50      6.39G     0.8311     0.5294     0.9482       1876        640: 0% ──────────── 0/76  0.3s"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K      19/50      7.45G     0.8709      0.553       1.01        792        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:11\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.5it/s 2.7s\n",
            "                   all        120       5922      0.255       0.26      0.299      0.215\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/50      3.08G     0.9082     0.5677     0.9932       2391        640: 0% ──────────── 0/76  0.5s"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K      20/50      5.22G     0.8852     0.5528      1.004        791        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:12\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.3it/s 1.7s\n",
            "                   all        120       5922      0.225       0.21      0.232      0.164\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      21/50      5.22G     0.8671     0.5568      1.005       1618        640: 0% ──────────── 0/76  1.1s"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K      21/50      5.22G     0.8636     0.5384     0.9976        838        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:12\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.1it/s 1.9s\n",
            "                   all        120       5922      0.325      0.255       0.29      0.211\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      22/50      5.22G     0.8011     0.5375      1.017       1569        640: 0% ──────────── 0/76  0.6s"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K      22/50      6.14G     0.8753     0.5417      1.001        725        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:12\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.1it/s 1.9s\n",
            "                   all        120       5922        0.3      0.327      0.311      0.231\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      23/50      6.14G      0.841     0.5077     0.9571       2032        640: 0% ──────────── 0/76  0.3s"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K      23/50      6.14G     0.8553     0.5257     0.9931        820        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:12\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.2it/s 1.8s\n",
            "                   all        120       5922      0.258      0.281      0.321      0.236\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      24/50      6.14G     0.8196     0.4993     0.9898       1403        640: 0% ──────────── 0/76  0.4s"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      24/50      6.14G     0.8582     0.5241     0.9875       1167        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:11\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.5it/s 2.6s\n",
            "                   all        120       5922      0.285      0.342      0.289      0.212\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      25/50      6.14G     0.9029     0.5214     0.9777       2302        640: 0% ──────────── 0/76  0.9s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      25/50      6.14G     0.8465     0.5197     0.9885        815        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:11\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.1it/s 1.9s\n",
            "                   all        120       5922      0.273      0.304      0.258      0.196\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      26/50      6.14G     0.8194     0.5122     0.9434       1790        640: 0% ──────────── 0/76  0.6s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      26/50      6.14G     0.8494     0.5212     0.9866        699        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:12\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.3it/s 1.7s\n",
            "                   all        120       5922      0.281      0.347      0.307       0.23\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      27/50      6.14G      0.857     0.5169     0.9676       1835        640: 0% ──────────── 0/76  0.6s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      27/50      6.14G      0.852     0.5138     0.9832       1034        640: 100% ━━━━━━━━━━━━ 76/76 1.0it/s 1:13\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.5it/s 1.6s\n",
            "                   all        120       5922      0.275      0.356      0.305      0.227\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      28/50      6.14G     0.8619     0.5086     0.9939       1897        640: 0% ──────────── 0/76  1.0s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      28/50      6.14G     0.8399      0.512     0.9791        895        640: 100% ━━━━━━━━━━━━ 76/76 1.0it/s 1:13\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.2it/s 1.8s\n",
            "                   all        120       5922       0.29      0.347      0.326      0.242\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      29/50      6.15G     0.7866      0.506      0.951       1343        640: 0% ──────────── 0/76  0.3s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      29/50      6.16G     0.8325     0.5052     0.9723        949        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:11\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s\n",
            "                   all        120       5922      0.244      0.283       0.28      0.211\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      30/50      6.17G     0.8478     0.5234     0.9962       1635        640: 0% ──────────── 0/76  0.4s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      30/50      6.18G     0.8353     0.5018     0.9738        801        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:11\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.3it/s 1.8s\n",
            "                   all        120       5922      0.288       0.33        0.3      0.228\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      31/50      6.18G     0.7907     0.4871       0.97       1286        640: 0% ──────────── 0/76  0.3s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      31/50       6.2G     0.8305     0.5006     0.9726        796        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:12\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 3.0it/s 1.3s\n",
            "                   all        120       5922      0.313      0.325      0.339      0.257\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      32/50       6.2G     0.8422     0.4928     0.9628       2123        640: 0% ──────────── 0/76  1.2s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      32/50      6.21G     0.8331      0.499     0.9695        897        640: 100% ━━━━━━━━━━━━ 76/76 1.0it/s 1:12\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.1it/s 1.9s\n",
            "                   all        120       5922      0.295      0.338      0.319      0.244\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      33/50      6.22G     0.7888     0.4726     0.9544       1676        640: 0% ──────────── 0/76  0.4s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      33/50      6.23G     0.8258     0.4899     0.9703        864        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:11\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.5s\n",
            "                   all        120       5922      0.297      0.421      0.354      0.272\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      34/50      6.24G     0.8818     0.5222     0.9754       2132        640: 0% ──────────── 0/76  0.5s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      34/50      6.24G     0.8256     0.4894     0.9656        996        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:11\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.3it/s 1.7s\n",
            "                   all        120       5922      0.306      0.351      0.336      0.253\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      35/50      6.25G     0.8121     0.4929     0.9764       1699        640: 0% ──────────── 0/76  1.0s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      35/50      7.19G      0.817     0.4845     0.9648        639        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:12\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.1it/s 1.9s\n",
            "                   all        120       5922      0.346      0.389      0.362      0.274\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      36/50       7.2G      0.808     0.4858     0.9474       1443        640: 0% ──────────── 0/76  0.7s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      36/50      8.26G     0.8193     0.4823     0.9618        919        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:12\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.3it/s 1.7s\n",
            "                   all        120       5922      0.355      0.393      0.358      0.273\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      37/50      3.07G     0.7921     0.4641     0.9425       1799        640: 0% ──────────── 0/76  0.3s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      37/50      5.03G     0.8193     0.4805     0.9648        955        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:12\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.1it/s 1.9s\n",
            "                   all        120       5922       0.33      0.367      0.364      0.283\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      38/50      5.03G     0.8659     0.4848      0.971       1997        640: 0% ──────────── 0/76  0.5s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      38/50      5.03G     0.8163     0.4764     0.9608       1097        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:11\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.4it/s 2.8s\n",
            "                   all        120       5922      0.321      0.367      0.344      0.264\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      39/50      5.03G     0.8572     0.4988     0.9841       1974        640: 0% ──────────── 0/76  0.4s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      39/50      5.03G     0.8072     0.4725     0.9574        737        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:11\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.5it/s 1.6s\n",
            "                   all        120       5922      0.332      0.346      0.344      0.266\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      40/50      5.03G     0.8331     0.4769     0.9722       1961        640: 0% ──────────── 0/76  1.0s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      40/50      5.94G     0.8187     0.4808     0.9604       1137        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:12\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.3it/s 1.7s\n",
            "                   all        120       5922      0.322      0.371      0.351       0.27\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      41/50      5.94G     0.8425     0.5336     0.9705       1054        640: 0% ──────────── 0/76  0.6s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      41/50      5.94G     0.8074     0.4819     0.9712        649        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:12\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.3it/s 1.8s\n",
            "                   all        120       5922      0.349      0.426      0.378      0.299\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      42/50      5.94G     0.7493      0.455     0.9465        774        640: 0% ──────────── 0/76  0.6s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      42/50      5.94G      0.791     0.4635     0.9623        392        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:12\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.3it/s 1.7s\n",
            "                   all        120       5922      0.387      0.396      0.371      0.295\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      43/50      5.94G     0.7842     0.4291     0.9739        955        640: 0% ──────────── 0/76  0.6s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      43/50      5.94G     0.7821      0.455     0.9674        356        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:12\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.1it/s 1.9s\n",
            "                   all        120       5922      0.395      0.393      0.393      0.314\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      44/50      5.94G     0.7367     0.4587     0.9753        908        640: 0% ──────────── 0/76  0.3s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      44/50      5.94G      0.781      0.452     0.9675        466        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:11\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.5it/s 2.7s\n",
            "                   all        120       5922      0.403      0.399      0.395      0.316\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      45/50      5.94G     0.7764     0.4359     0.9368        978        640: 0% ──────────── 0/76  0.4s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      45/50      5.94G     0.7791     0.4489     0.9617        495        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:10\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.3it/s 1.7s\n",
            "                   all        120       5922      0.383      0.388      0.367      0.292\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      46/50      5.94G     0.8167     0.4741     0.9203       1087        640: 0% ──────────── 0/76  0.4s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      46/50      5.94G     0.7766      0.447      0.959        430        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:11\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.4it/s 1.6s\n",
            "                   all        120       5922      0.348      0.371      0.368      0.293\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      47/50      5.94G     0.8125     0.4504     0.9822       1101        640: 0% ──────────── 0/76  0.4s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      47/50      5.94G     0.7762     0.4419      0.964        459        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:11\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.1it/s 1.9s\n",
            "                   all        120       5922      0.386      0.402      0.377      0.298\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      48/50      5.94G     0.7202     0.4364     0.9256       1029        640: 0% ──────────── 0/76  0.9s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      48/50      5.95G     0.7764     0.4438      0.954        480        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:11\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.4it/s 1.6s\n",
            "                   all        120       5922      0.405      0.435      0.389      0.312\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      49/50      5.96G     0.7785     0.4388     0.9869       1002        640: 0% ──────────── 0/76  1.0s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      49/50      5.96G     0.7709     0.4398     0.9582        554        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:12\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.3it/s 1.7s\n",
            "                   all        120       5922      0.369      0.386      0.389      0.311\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      50/50      5.97G      0.824     0.4721     0.9617       1069        640: 0% ──────────── 0/76  0.5s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:157.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      50/50      5.98G     0.7728     0.4401     0.9585        429        640: 100% ━━━━━━━━━━━━ 76/76 1.1it/s 1:11\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.4it/s 1.7s\n",
            "                   all        120       5922      0.364      0.387      0.386      0.308\n",
            "\n",
            "50 epochs completed in 1.046 hours.\n",
            "Optimizer stripped from /content/runs/detect/malaria_attention_run/weights/last.pt, 6.3MB\n",
            "Optimizer stripped from /content/runs/detect/malaria_attention_run/weights/best.pt, 6.3MB\n",
            "\n",
            "Validating /content/runs/detect/malaria_attention_run/weights/best.pt...\n",
            "Ultralytics 8.3.246 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLOv8_cbam summary (fused): 80 layers, 3,015,303 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7s/it 6.7s\n",
            "                   all        120       5922      0.402        0.4      0.394      0.317\n",
            "        red blood cell        120       5614      0.885      0.909      0.933      0.764\n",
            "           trophozoite         59        111      0.416      0.872      0.605      0.433\n",
            "             difficult          5          5          0          0     0.0136    0.00592\n",
            "                  ring         57        169      0.493       0.27       0.33      0.283\n",
            "              schizont          9         11      0.185      0.182      0.148      0.116\n",
            "            gametocyte         11         12      0.435      0.167      0.337      0.298\n",
            "Speed: 0.5ms preprocess, 5.2ms inference, 0.0ms loss, 13.1ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/malaria_attention_run\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# CELL 6: Platelet-Style \"True vs Pred\" Counting\n",
        "# ==========================================\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "test_images = 'datasets/malaria_yolo/images/test'\n",
        "test_labels = 'datasets/malaria_yolo/labels/test'\n",
        "\n",
        "# Load best model\n",
        "best_model = YOLO('runs/detect/malaria_attention_run/weights/best.pt')\n",
        "\n",
        "print(\"\\n\" + \"=\"*95)\n",
        "print(f\"{'Image Name':<40} | {'True Objects':<12} | {'Pred Objects':<12} | {'Diff':<5}\")\n",
        "print(\"=\"*95)\n",
        "\n",
        "total_true = 0\n",
        "total_pred = 0\n",
        "\n",
        "# Just check the first 20 images to avoid spamming output, remove slice [:20] for full\n",
        "image_files = glob.glob(os.path.join(test_images, '*.*'))\n",
        "\n",
        "for img_file in image_files[:30]:\n",
        "    filename = os.path.basename(img_file)\n",
        "    label_file = os.path.join(test_labels, filename.rsplit('.', 1)[0] + '.txt')\n",
        "\n",
        "    # 1. Get True Count (excluding RBCs usually, or counting all)\n",
        "    # Note: Class 0 is RBC. In Malaria detection, we usually care about Parasites (Classes 1,3,4,5)\n",
        "    # Let's count TOTAL objects for general detection, or modify 'if' below to filter\n",
        "    true_count = 0\n",
        "    if os.path.exists(label_file):\n",
        "        with open(label_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            # To count ONLY parasites (exclude RBC/Leukocyte), uncomment next line:\n",
        "            # lines = [l for l in lines if l.split()[0] not in ['0', '6']]\n",
        "            true_count = len(lines)\n",
        "\n",
        "    # 2. Predict\n",
        "    # conf=0.25 is standard confidence threshold\n",
        "    results = best_model.predict(img_file, verbose=False, conf=0.25)\n",
        "\n",
        "    # Filter predictions same as ground truth if needed\n",
        "    # boxes.cls gives class IDs.\n",
        "    pred_boxes = results[0].boxes\n",
        "    # To count ONLY parasites:\n",
        "    # pred_count = len([c for c in pred_boxes.cls if int(c) not in [0, 6]])\n",
        "    pred_count = len(pred_boxes)\n",
        "\n",
        "    # 3. Stats\n",
        "    diff = pred_count - true_count\n",
        "    total_true += true_count\n",
        "    total_pred += pred_count\n",
        "\n",
        "    print(f\"{filename:<40} | {true_count:<12} | {pred_count:<12} | {diff:<5}\")\n",
        "\n",
        "print(\"=\"*95)\n",
        "print(f\"Total (Sample): True={total_true} | Predicted={total_pred}\")\n",
        "if total_true > 0:\n",
        "    print(f\"Error Rate: {abs(total_true - total_pred) / total_true:.2%}\")\n",
        "\n",
        "# ==========================================\n",
        "# CELL 7: Download Runs Folder\n",
        "# ==========================================\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Define the path to the 'runs' folder\n",
        "runs_folder_path = 'runs'\n",
        "output_zip_name = 'runs_folder.zip'\n",
        "\n",
        "# Create a zip archive of the 'runs' folder\n",
        "print(f\"Compressing '{runs_folder_path}' into '{output_zip_name}'...\")\n",
        "# Correcting the base_name for make_archive to ensure the output file has the desired name\n",
        "shutil.make_archive(output_zip_name.rsplit('.', 1)[0], 'zip', runs_folder_path)\n",
        "\n",
        "# Provide the download link\n",
        "print(f\"'{output_zip_name}' created. Initiating download...\")\n",
        "files.download(output_zip_name)\n",
        "\n",
        "print(\"Download initiated. Please check your browser's download location.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "id": "m5BDK92te_e5",
        "outputId": "52d6e11a-f74b-4bb3-e1fa-502dad2ae512"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================================================================\n",
            "Image Name                               | True Objects | Pred Objects | Diff \n",
            "===============================================================================================\n",
            "bd4da4f8-8996-47e0-a4e5-a3da5717441e.jpg | 13           | 17           | 4    \n",
            "010961af-b38c-49de-aca0-e3732d73d414.jpg | 58           | 64           | 6    \n",
            "9cf0b006-cb5c-47e7-b076-1dcacf1fbfb1.jpg | 50           | 59           | 9    \n",
            "f0ee03d6-c57b-43a0-8812-359330bdb93a.jpg | 104          | 47           | -57  \n",
            "5a2447bf-a92b-4a47-a384-4b2ff020ecdc.jpg | 39           | 36           | -3   \n",
            "03be08c8-2cf3-4de6-8ac6-5f7817a6a643.jpg | 55           | 61           | 6    \n",
            "baab0701-3fd4-4940-8b90-b9c42d95888f.jpg | 70           | 70           | 0    \n",
            "e26b88c6-5884-4b5c-94d7-ad780379c8ad.jpg | 70           | 78           | 8    \n",
            "b1d312cf-1f52-4955-bf42-58d3664c254a.jpg | 50           | 56           | 6    \n",
            "c8d1b7ec-be9f-4c88-a39b-2c6bf46b2998.jpg | 47           | 52           | 5    \n",
            "efb85f8e-c780-4a19-b6ce-555a154b0d10.jpg | 84           | 77           | -7   \n",
            "8874ea02-d263-4830-99d1-e1b14230a56b.jpg | 49           | 57           | 8    \n",
            "738b7be5-59b5-4d8c-8769-cbf2385b9fc7.jpg | 52           | 53           | 1    \n",
            "b910c1db-9c66-4207-892f-18a03ecc75bc.jpg | 60           | 66           | 6    \n",
            "693da701-2020-404c-866d-b64c12da951d.jpg | 36           | 38           | 2    \n",
            "3e03b313-66eb-471f-a97a-7b2bf20dcff5.jpg | 64           | 63           | -1   \n",
            "96287315-87a1-4045-9e16-119e045bf12a.jpg | 49           | 54           | 5    \n",
            "2c03b3c9-7786-4e1b-ac2b-c98e9f8358f2.jpg | 34           | 30           | -4   \n",
            "51a42643-256f-463a-bf22-7fd295caead0.jpg | 79           | 69           | -10  \n",
            "9fd32265-07f2-442b-80b5-35069891d452.jpg | 47           | 50           | 3    \n",
            "f8ebe0c8-a90d-4a64-9f6a-f9985e38c340.jpg | 62           | 30           | -32  \n",
            "55078c19-bfaf-4988-a779-767e1c6d22f6.jpg | 38           | 39           | 1    \n",
            "96957ec3-de17-41cf-855b-1388814e5918.jpg | 57           | 58           | 1    \n",
            "8e22f8cc-b6ef-48cb-a06d-a44cb1fb5f2f.jpg | 32           | 29           | -3   \n",
            "1b814efc-7bf9-40ae-8e13-49dc2c04732f.jpg | 38           | 40           | 2    \n",
            "31c62771-4e90-4038-9abc-d196beda9898.jpg | 24           | 25           | 1    \n",
            "41be1bd3-0d31-4881-bf1f-3ccdfa21ff12.jpg | 67           | 75           | 8    \n",
            "087602a2-288b-4a4d-b01a-524f0d799d89.jpg | 50           | 47           | -3   \n",
            "446734b7-6bc9-4a66-a3a0-49f6aa90db33.jpg | 71           | 60           | -11  \n",
            "96afe167-6a2f-4770-aa74-e083fbd5e926.jpg | 26           | 25           | -1   \n",
            "===============================================================================================\n",
            "Total (Sample): True=1575 | Predicted=1525\n",
            "Error Rate: 3.17%\n",
            "Compressing 'runs' into 'runs_folder.zip'...\n",
            "'runs_folder.zip' created. Initiating download...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4db86078-de31-49c0-b78e-b3d40a1e5577\", \"runs_folder.zip\", 23274573)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download initiated. Please check your browser's download location.\n"
          ]
        }
      ]
    }
  ]
}